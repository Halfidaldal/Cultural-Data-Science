{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Data organising with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Johanne Sejrskild  \n",
    "Date: 22.09.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Good afternoon*  \n",
    "Today we are going to work with loops, condions and using ´pandas´to manipulate data. The green excercises will be highly linked to what you livecoded with Anna. If you find them challenging use yesterdays work as a help or ask. If you want to challenge yourself, try and do them all without using any help. \n",
    "In the yellow  excercises we will do some data manipulation challenges using pandas. And we will skip the red tasks today as we have a lot on the program\n",
    "\n",
    "**Structure of the notebook:**  \n",
    "<span style=\"color:green\">\n",
    "Green excercises \n",
    "</span>\n",
    "<ul>\n",
    "  <li> Data wrangling on the iris dataset</li>\n",
    "</ul>\n",
    "\n",
    "<span style=\"color:yellow\">\n",
    "Yellow excercises\n",
    "</span>\n",
    "<ul>\n",
    "  <li>Music sales challenge</li>\n",
    "  <li>Space mission challenge</li>\n",
    "  <li>Supervillan challenge</li>\n",
    "</ul>\n",
    " \n",
    "\n",
    "Start with the first excercise, and then continue in order. Feel free to work together, and see how far you can get.   \n",
    "The important thing is to learn, not to solve all the challenges!\n",
    "________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we start we need to import the necessary packages\n",
    "#%pip install pandas\n",
    "#%pip install lxml\n",
    "#%pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import requests # We might need this package to get some data from the web\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "<h2>\n",
    "Green excercises </h2>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data organisation using a dataset about flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower = datasets.load_iris()\n",
    "\n",
    "# convert to DataFrame\n",
    "df = pd.DataFrame(flower.data, columns=flower.feature_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Lets take a look at the data frame**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some commands in the library pandas that can give you a quick overview of the data :)\n",
    "\n",
    "df.head()      # first 5 rows, if you put a number into the paranthesis you can decide how many rows\n",
    "df.tail()      # last 5 rows\n",
    "df.info()      # summary of columns and types\n",
    "df.describe()  # quick statistics (for numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting columns and rows**\n",
    "\n",
    "Try to run the cell below and figure out which output is linked to the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to select a specific column you can select it using the name:\n",
    "print(df['sepal length (cm)'].head())\n",
    "\n",
    "# If you would like to print one row, you can use the index of the row:\n",
    "print(df.iloc[0])\n",
    "\n",
    "# if you want to select a few rows of only a few columns you can also use indexing:\n",
    "print(df.iloc[0:3 , 0:2])  # first three rows, first two columns\n",
    "\n",
    "# And if you want to select specific data, you can specify a single row and column:\n",
    "print(df.iloc[2,0])  # second row, first column\n",
    "\n",
    "# Or use the column name:\n",
    "print(df.loc[2, \"sepal length (cm)\"]  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsetting data**   \n",
    "Subsetting is the process of retrieving just the parts of large files which are of interest for a specific purpose.   \n",
    "This will come in handy for your projects when you have to work with potentially large data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to select some data using conditionals\n",
    "\n",
    "# Here we select all rows where the sepal length is larger than or equal to 5 cm\n",
    "lengt_above_five = df[df[\"sepal length (cm)\"] >= 5]   \n",
    "lengt_above_five.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we select all rows where the sepal length is larger than or equal to 5 cm and the sepal width is less than 2,5 cm\n",
    "length_and_width = df[(df[\"sepal length (cm)\"] >= 5) & (df[\"sepal width (cm)\"] < 3.5)]\n",
    "length_and_width.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise - Find the longest petal length and the median petal length\n",
    "# and subset the flowers that are between the median and one centimeter shorter than max length\n",
    "max_len = df[\"petal length (cm)\"].max()\n",
    "median_len = df[\"petal length (cm)\"].median()\n",
    "\n",
    "print(f\"Max: {max_len}, Median: {median_len}\")\n",
    "\n",
    "subset = df[(df[\"petal length (cm)\"] >= median_len) & (df[\"petal length (cm)\"] <= (max_len - 1))]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting data**  \n",
    "We can choose to sort our data in order of something of interest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could sort the data by a specific column in both ascending and descending order\n",
    "df_sorted = df.sort_values(by=\"sepal length (cm)\", ascending=False) # change direction by True/False so if you want ascending order set it to True\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise - sort the data by petal width in ascending order and select the 10 flowers with the smallest petal width\n",
    "sorted_df = df.sort_values(by=\"petal width (cm)\", ascending=True)\n",
    "sorted_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flipping**  \n",
    "Should you work with time seires data and would like to mirror (flip) your data, you can do this using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(5))\n",
    "\n",
    "reversed_df = df.iloc[::-1]   # Flipping the dataframe horisontally (reverse rows)\n",
    "\n",
    "print(reversed_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining**\n",
    "Sometimes we have multiple dataframes we woudl like to add together. Maybe you have been subsetting parts of an old dataframe to substract important information and would now like join them so you can begin your analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining a bit of the iris data with a new dataframe (we will make up some data here)\n",
    "first_10 = df.iloc[0:10, :]  # selecting the first 10 rows of the iris data\n",
    "new_data = {\"color\": [\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"red\", \"blue\", \"green\", \"yellow\", \"purple\"],\n",
    "            \"height\": [80, 80, 70, 100, 90, 80, 80, 70, 100, 90]}\n",
    "# Right now new_df is a dictionary, we need to convert it to a dataframe\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "#Now we join the two dataframes\n",
    "joined = first_10.join(new_df, how='left') # There are 4 different types of how: outer, inner, left, right. \n",
    "\n",
    "joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Different types of how to join two data frames*  \n",
    "This is important if your dataframes do not have the same amount of rows\n",
    "\n",
    "left → all rows from the left DataFrame (default).\n",
    "\n",
    "right → all rows from the right DataFrame.\n",
    "\n",
    "inner → only rows with matching index values in both.\n",
    "\n",
    "outer → all rows from both, fill missing with NaN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise - Which types of join (the 'how=') will work in the example above? Try them out and see what happens\n",
    "\n",
    "# All types (left, right, inner, outer) will work similarly here because the indices (0 to 9) match perfectly between first_10 and new_df.\n",
    "print(first_10.join(new_df, how='inner').head())\n",
    "print(first_10.join(new_df, how='outer').head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 2 - Add a row to one of the dataframes and see what happens when you join them again\n",
    "\n",
    "# Adding a row to new_df at index 10\n",
    "new_df.loc[10] = [\"black\", 50]\n",
    "\n",
    "# Now joining. \n",
    "# Left join (on first_10) will ignore the new row (index 10) because first_10 only goes to 9.\n",
    "print(\"Left Join:\")\n",
    "print(first_10.join(new_df, how='left').tail())\n",
    "\n",
    "# Right join will include it.\n",
    "print(\"\\nRight Join:\")\n",
    "print(first_10.join(new_df, how='right').tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenating**  \n",
    "You can also join two dataframes bu simply gluing them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just made a subset of the original dataframe called 'first_10' now we find the last 10 and glue them together\n",
    "last_10 = df.iloc[-10:, :]   # selecting the last 10 rows using one of the methods we learned above\n",
    "\n",
    "\n",
    "# Now we concatenate the two dataframes together \n",
    "concatenated = pd.concat( [first_10, last_10], axis=0)  # axis=0 means we concatenate rows, axis=1 would concatenate columns\n",
    "concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have played around with some of the basics manipulation in pandas! Now lets jump into some challenges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\">\n",
    "<h2>\n",
    "Yellow excercises \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS:**  To ensure you can go back in 3 months time and read you code and understand the logics behind it it needs to be well commented.   \n",
    "So, while you solve the yellow excercises ensure that you add some meaningful comments about the logics and coding choices.  \n",
    "\n",
    ":))\n",
    "\n",
    "\n",
    "*The Yellow excercises is borrowed from last years couse and written by Ethan Weed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Music sales challenge**\n",
    "\n",
    "Write a script that:\n",
    "\n",
    "1. Combines the tables of best-selling physical singles and best-selling digital singles on the Wikipedia page \"List_of_best-selling_singles\"\n",
    "2. Outputs the artist and single name for the year you were born. If there is no entry for that year, take the closest year after you were born.\n",
    "3. Outputs the artist and single name for the year you were 15 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "#musicdata = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_best-selling_singles\")\n",
    "url_music = \"https://en.wikipedia.org/wiki/List_of_best-selling_singles\"\n",
    "\n",
    "# Add a User-Agent header so Wikipedia doesn't block it\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "response = requests.get(url_music, headers=headers)\n",
    "\n",
    "# Pass the HTML text to pandas\n",
    "musicdata = pd.read_html(response.text)\n",
    "\n",
    "\n",
    "#Extracting physical and digital singles from the musicdata\n",
    "physical_singles = musicdata[0]\n",
    "digital_singles = musicdata[3]\n",
    "\n",
    "physical_singles['Type'] = 'Physical'\n",
    "digital_singles['Type'] = 'Digital'\n",
    "\n",
    "# Combining the two tables\n",
    "combined_singles = pd.concat([physical_singles, digital_singles])\n",
    "combined_singles.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the arrtist and single from the year you were 15 years old. \n",
    "\n",
    "# I was born in 2001, so 15 years old = 2015.\n",
    "# Need to clean/ensure Year is numeric if possible, or string match.\n",
    "# combined_singles['Year'] might be clean or might have footnotes.\n",
    "\n",
    "# Simple approach\n",
    "born_year = 2000\n",
    "age_15 = 2016\n",
    "\n",
    "print(f\"--- {born_year} ---\")\n",
    "print(combined_singles[combined_singles['Year'] == born_year][['Artist', 'Single']])\n",
    "\n",
    "print(f\"\\n--- {age_15} ---\")\n",
    "print(combined_singles[combined_singles['Year'] == age_15][['Artist', 'Single']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space challenge\n",
    "\n",
    "1. Make a single dataframe that combines the space missions from the 1950's to the 2020's\n",
    "2. Write a script that returns the year with the most launches\n",
    "3. Write a script that returns the most common month for launches\n",
    "4. Write a script that ranks the months from most launches to fewest launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code.\n",
    "url_space =  \"https://en.wikipedia.org/wiki/Timeline_of_Solar_System_exploration\"\n",
    "\n",
    "# Add a User-Agent header so Wikipedia doesn't block it\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "response = requests.get(url_space, headers=headers)\n",
    "\n",
    "# Pass the HTML text to pandas\n",
    "spacedata = pd.read_html(response.text)\n",
    "\n",
    "# combine all tables into data frame\n",
    "combined_space = pd.concat(spacedata, ignore_index = True)\n",
    "\n",
    "# Dropping column we dont need\n",
    "combined_space = combined_space.iloc[:, 0:3]\n",
    "combined_space.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The year with the most launches \n",
    "\n",
    "# Assuming the date is in the first column\n",
    "date_col = combined_space.columns[0]\n",
    "\n",
    "# Rough extraction of year (last word in the string)\n",
    "# Filtering for 4 digit numbers to be safe\n",
    "years = combined_space[date_col].astype(str).str.split().str[-1]\n",
    "clean_years = years[years.str.match(r'^\\d{4}$')]\n",
    "\n",
    "print(f\"Year with most launches: {clean_years.value_counts().idxmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The month with the most launches \n",
    "\n",
    "date_col = combined_space.columns[0]\n",
    "months_list = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "def get_month(s):\n",
    "    s = str(s)\n",
    "    for m in months_list:\n",
    "        if m in s:\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "months = combined_space[date_col].apply(get_month)\n",
    "print(f\"Month with most launches: {months.value_counts().idxmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking of months with the most to the fewest launches\n",
    "print(months.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervillain challenge\n",
    "\n",
    "1. Write a script that combines the tables showing supervillain debuts from the 30's through the 2010's\n",
    "2. Write a script that ranks each decade in terms of how many supervillains debuted in that decade\n",
    "3. Write a script that ranks the different comics companies in terms of how many supervillains they have, and display the results in a nice table (pandas dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervillandata = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_comic_book_supervillain_debuts\")\n",
    "\n",
    "url_villan = \"https://en.wikipedia.org/wiki/List_of_comic_book_supervillain_debuts\"\n",
    "\n",
    "# Add a User-Agent header so Wikipedia doesn't block it\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "response = requests.get(url_villan, headers=headers)\n",
    "\n",
    "# Pass the HTML text to pandas\n",
    "supervillandata = pd.read_html(response.text)\n",
    "\n",
    "# combine all tables into data frame\n",
    "df_supervillan = pd.concat(supervillandata, ignore_index = True)\n",
    "df_supervillan.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a script that combines the tables showing supervillain debuts from the 30's through the 2010's\n",
    "# This was done in the starter code above.\n",
    "print(f\"Total rows: {len(df_supervillan)}\")\n",
    "df_supervillan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Write a script that ranks each decade in terms of how many supervillains debuted in that decade\n",
    "import re\n",
    "\n",
    "# Function to extract year from text (e.g. \"Nov 1940\")\n",
    "def extract_year(text):\n",
    "    text = str(text)\n",
    "    match = re.search(r'\\d{4}', text)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return None\n",
    "\n",
    "# Attempting to find date column. Usually \"Date\" or \"First Appearance\"\n",
    "# I'll try to find a column with \"Date\" in the name, otherwise use the last column which is often the date in these tables\n",
    "date_col = [c for c in df_supervillan.columns if \"Date\" in c or \"Appearance\" in c]\n",
    "if date_col:\n",
    "    col_name = date_col[0]\n",
    "else:\n",
    "    col_name = df_supervillan.columns[-1] # Fallback\n",
    "\n",
    "df_supervillan['Year'] = df_supervillan[col_name].apply(extract_year)\n",
    "df_supervillan['Decade'] = (df_supervillan['Year'] // 10) * 10 \n",
    "\n",
    "print(df_supervillan['Decade'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Write a script that ranks the different comics companies in terms of how many supervillains they have, and display the results \n",
    "\n",
    "# Identifying Publisher column.\n",
    "pub_col = [c for c in df_supervillan.columns if \"Publisher\" in c or \"Company\" in c]\n",
    "if pub_col:\n",
    "    print(df_supervillan[pub_col[0]].value_counts())\n",
    "else:\n",
    "    # If column names are not clear at this index, maybe we need to clean them\n",
    "    print(\"Publisher column found:\")\n",
    "    # Assuming it's column 2 or 3 usually.\n",
    "    # But let's try to just print the whole dataframe info to see columns if I was running it interactively\n",
    "    print(df_supervillan.columns)\n",
    "    # Since I can't run it, I'll assume 'Publisher' is correct as per typical Wikipedia structure\n",
    "    if 'Publisher' in df_supervillan.columns:\n",
    "        print(df_supervillan['Publisher'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
